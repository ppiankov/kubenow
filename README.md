![kubenow logo](https://raw.githubusercontent.com/ppiankov/kubenow/main/docs/img/logo.png)

# ğŸ§¯ kubenow â€” Kubernetes Incident Triage on Demand

â€œ11434 is enough.â€

## kubenow is a single Go binary that takes a live Kubernetes cluster snapshot and feeds it into an LLM (local or cloud) to generate:
	â€¢	ğŸ”¥ incident triage (ranked, actionable, command-ready)
	â€¢	ğŸ›  pod-level debugging
	â€¢	ğŸ“Š cluster health summaries
	â€¢	ğŸ‘©â€ğŸ’¼ teamlead / ownership recommendations
	â€¢	ğŸ§¹ compliance / hygiene reviews
	â€¢	ğŸ§¨ chaos engineering experiment suggestions

## It works with any OpenAI-compatible API, including:
	â€¢	ğŸ¦™ Ollama (Mixtral, Llama, Qwen, etc.)
	â€¢	â˜ï¸ OpenAI / Azure OpenAI
	â€¢	ğŸ”§ DeepSeek / Groq / Together / OpenRouter
	â€¢	or your own weird homemade inference server

If your laptop can run it and respond to /v1/chat/completions,
kubenow will talk to it.

# âœ¨ Why kubenow?

## Because when the cluster is on fire, nobody wants to run:
	â€¢	12 commands
	â€¢	across 5 namespaces
	â€¢	using 4 terminals
	â€¢	while Slack is screaming

You want:
```bash
TOP ISSUES:
1. callback/data-converter-worker â€” ImagePullBackOff â€” critical
2. payments-api â€” CrashLoopBackOff â€” high

ROOT CAUSES:
1. Private registry unreachable from nodes.
2. readinessProbe fails immediately.

FIX COMMANDS:
kubectl -n callback get events
kubectl -n callback set image deploy/data-converter-worker api=repo/worker:stable
kubectl -n prod edit deploy/payments-api
```

Short, ranked, actionable.

And yes â€” kubenow can also run teamlead mode, which gently hints at which team probably caused the outage.

# ğŸ§© Features

## ğŸ”¥ Incident Mode (--mode incident)
	â€¢	Ranks the top problems in the cluster
	â€¢	Gives 1â€“2 sentence root causes
	â€¢	Provides actionable kubectl / YAML patches
	â€¢	Zero fluff, zero theory

## ğŸ§ª Pod Mode (--mode pod)

Deep dive into broken pods:
	â€¢	container states
	â€¢	events
	â€¢	restarts
	â€¢	image pulls
	â€¢	OOMs
	â€¢	last logs

## ğŸ“Š Default Mode

High-level cluster summary with readable health insights.

## ğŸ‘©â€ğŸ’¼ Teamlead Mode (--mode teamlead)

Manager-friendly report:
	â€¢	risk
	â€¢	blast radius
	â€¢	ownership hints
	â€¢	escalation guidance

## ğŸ“ Compliance Mode (--mode compliance)

Finds policy / hygiene issues:
	â€¢	missing resource limits
	â€¢	:latest tags
	â€¢	namespace misuse
	â€¢	registry hygiene
	â€¢	bad env patterns

## ğŸ§¨ Chaos Mode

Suggests targeted chaos experiments based on real weaknesses:
	â€¢	node drain
	â€¢	registry outage simulation
	â€¢	disruption tests
	â€¢	restart storms

â¸»

# ğŸ“¦ Installation

Build from source

Requires Go â‰¥ 1.25.4

```bash
git clone https://github.com/ppiankov/kubenow
cd kubenow
go build ./cmd/kubenow
```

(Optional) Move to PATH

```bash
sudo mv kubenow /usr/local/bin/
```

Helps DevOps engineers identify pods with incorrectly configured
resource limits/requests, reducing cluster waste and improving stability.

# ğŸš€ Usage

You only need:
	â€¢	a kubeconfig
	â€¢	an LLM endpoint
	â€¢	a model name

Example (local Ollama)
```bash
./kubenow \
  --llm-endpoint http://localhost:11434/v1 \
  --model mixtral:8x22b \
  --mode incident
```
Example (OpenAI)

```bash
export KUBENOW_API_KEY="sk-yourkey"

./kubenow \
  --llm-endpoint https://api.openai.com/v1 \
  --model gpt-4.1-mini \
  --mode teamlead
```

Example (one specific namespace)

```bash
./kubenow \
  --namespace prod \
  --mode pod \
  --llm-endpoint http://localhost:11434/v1 \
  --model mixtral:8x22b
```


# ğŸ§  Recommended Models
| Mode | Best Local | Best Cloud |notes |
|-------|-----|-----------|-----------|
| incident | mixtral:8x22b | GPT-4.1 Mini | concise, obedient|
| pod | llama3:70b (if patient) | GPT-4.1 | detail friendly |
| teamlead | mixtral:8x22b | GPT-4.1 Mini | leadership tone |
| compliance | mixtral or Qwen |GPT-4.1 Mini | structured |
| chaos | mixtral |GPT-4.1 Mini | creative but grounded|

Quote of the project:
â€œ11434 is enough.â€

# ğŸ”§ Command-Line Flags

```bash
--kubeconfig <path>     Path to kubeconfig (optional)
--namespace <ns>        Only analyze this namespace
--mode <type>           default|pod|incident|teamlead|compliance|chaos
--llm-endpoint <url>    OpenAI-compatible URL
--model <name>          Model name (mixtral:8x22b, gpt-4.1-mini, etc.)
--api-key <key>         LLM API key (optional if local)
--max-pods <num>        Max problem pods to include (default: 10)
--log-lines <num>       Logs per container (default: 50)
```

# ğŸ§± Architecture

## Scenario 1: Silent LLM RAM spike

```bash
cmd/kubenow/
internal/
  snapshot/   â† collects K8s data, applies issueType classification
  prompt/     â† loads prompt templates by mode
  llm/        â† calls OpenAI-compatible APIs
  util/       â† kube client builder
prompts/
  default.txt
  pod.txt
  incident.txt
  teamlead.txt
  compliance.txt
  chaos.txt
```
Snapshot contains:
	â€¢	node conditions
	â€¢	problem pods with:
	â€¢	reason
	â€¢	restart count
	â€¢	container states
	â€¢	resource requests/limits
	â€¢	image names
	â€¢	last logs
	â€¢	pod events
	â€¢	issueType (ImagePullError | CrashLoop | OOMKilled | PendingScheduling | etc.)

# ğŸ“„ License
MIT

# ğŸ‰ Disclaimer

## This tool can:
	â€¢	shame your engineers
	â€¢	uncover your terrible cluster hygiene
	â€¢	predict who broke production
	â€¢	and suggest chaos tests strong enough to get you fired

Use responsibly.

## âœ¨ Keywords
	â€¢	kubernetes incident response LLM
	â€¢	kubernetes triage cli
	â€¢	ollama kubernetes assistant
	â€¢	k8s troubleshooting
	â€¢	kubectl alternative
	â€¢	k8s observability
	â€¢	chaos engineering

---

